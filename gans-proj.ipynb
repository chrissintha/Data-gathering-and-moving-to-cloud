{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7911b2db-f85c-4568-944f-5848bf267c0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "GANS PROJECT\n",
    "For the Gans Project we need to work on getting information from avrious resources into our DB tables for further analysis\n",
    "Starting with Getting information about flights for major cities in germany. Using the AeroDataBoxAPI we get the flights information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f67e6-053b-4869-9c62-c05f249bb329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Flights API\n",
    "#Getting the arrivals for major cities in Germany form the AeroDataBoxAPI\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "\n",
    "#API key from the AeroDataBoxAPI\n",
    "API_key = \"API_KEY\"\n",
    "\n",
    "# Airports icoa\n",
    "list_airport_icoa = [\"EDDB\",\"EDDF\",\"EDDM\",\"EDDK\",\"EDDS\"]\n",
    "\n",
    "\n",
    "#company wants to know which flights will arrive the next day -Date Format.\n",
    "#Add 1 day.\n",
    "tomorrows_date = (datetime.now() + timedelta(days=1))\n",
    "tomorrows_date_from=tomorrows_date.strftime(\"%Y-%m-%dT%H:%M\")\n",
    "#add 11 hours\n",
    "future_date_and_time = tomorrows_date  + timedelta(hours=11)\n",
    "tomorrows_date_to = future_date_and_time.strftime(\"%Y-%m-%dT%H:%M\")\n",
    "def get_flight_info(flight_json,i):\n",
    "    # terminal\n",
    "    try: terminal = flight_json['arrival']['terminal']\n",
    "    except: terminal = None\n",
    "    # aircraft\n",
    "    try: aircraft = flight_json['aircraft']['model']\n",
    "    except: aircraft = None\n",
    "\n",
    "    return {\n",
    "        'dep_airport':flight_json['departure']['airport']['name'],\n",
    "        'sched_arr_loc_time':flight_json['arrival']['scheduledTimeLocal'],\n",
    "        'terminal':terminal,\n",
    "        'status':flight_json['status'],\n",
    "        'aircraft':aircraft,\n",
    "        'icao_code':i\n",
    "    }\n",
    "for i in list_airport_icoa:\n",
    "    print(i)\n",
    "    url = f\"https://aerodatabox.p.rapidapi.com/flights/airports/icao/{i}/{tomorrows_date_from}/{tomorrows_date_to}\"\n",
    "    querystring = {\"withLeg\":\"true\",\"withCancelled\":\"true\",\"withCodeshared\":\"true\",\n",
    "                   \"withCargo\":\"true\",\"withPrivate\":\"false\",\"withLocation\":\"false\"}\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": API_key\n",
    "    }\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    arrivals = response.json()['arrivals']\n",
    "    appended_data=pd.concat([appended_data, pd.DataFrame([get_flight_info(flight,i) for flight in arrivals])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be48c33-4202-4f6d-93fd-34916ef813be",
   "metadata": {
    "tags": []
   },
   "source": [
    "The API for openweathermap is used for getting the weather information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fb682-0b84-47e7-8e7c-356e4b5d582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather information\n",
    "from pyowm import OWM\n",
    "from pyowm.utils import config\n",
    "from pyowm.utils import timestamps\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "OWM_key = 'Own OWM key'\n",
    "\n",
    "city = [\"Berlin\", \"Cologne\", \"Frankfurt\", \"Munich\", \"Stuttgart\" ]\n",
    "country = \"DE\"\n",
    "\n",
    "forecast_api = []\n",
    "for i in city:\n",
    "    response = requests.get(f'http://api.openweathermap.org/data/2.5/forecast/?q={i},{country}&appid={OWM_key}&units=metric&lang=en')\n",
    "    forecast_api.append(response.json())\n",
    " # datetime, temperature, wind, prob_perc, rain_qty, snow \n",
    "weather_info = []\n",
    "for forecast_3h in forecast_api:\n",
    "    weather_hour = {'municipality_iso_country': forecast_3h['city']['name'] + ',' + forecast_3h['city']['country']}\n",
    "    for hour in forecast_3h['list']:\n",
    "        # datetime utc\n",
    "        weather_hour['datetime'] = hour['dt_txt']\n",
    "        # temperature \n",
    "        weather_hour['temperature'] = hour['main']['temp']\n",
    "        # wind\n",
    "        weather_hour['wind'] = hour['wind']['speed']\n",
    "        # probability precipitation \n",
    "        try: weather_hour['prob_perc'] = float(hour['pop'])\n",
    "        except: weather_hour['prob_perc'] = 0\n",
    "        # rain\n",
    "        try: weather_hour['rain_qty'] = float(hour['rain']['3h'])\n",
    "        except: weather_hour['rain_qty'] = 0\n",
    "        # wind \n",
    "        try: weather_hour['snow'] = float(hour['snow']['3h'])\n",
    "        except: weather_hour['snow'] = 0\n",
    "        weather_info.append(weather_hour.copy())\n",
    "        # weather_de = pd.concat(weather_de,pd.DataFrame.from_dict(weather_info), ignore_index=True)\n",
    "\n",
    "weather_de = pd.DataFrame(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8da5e-eb99-458f-93e6-e1f2aca0238a",
   "metadata": {},
   "source": [
    "Get the demographics information about the cities from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8e13c-f058-4132-84fa-e0ea0cf29348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Beautiful Soup to scrape data from the internet(Wikipedia pages for infomation on the cities)\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "cities = ['Berlin','Frankfurt','Stuttgart','Munich','Cologne']\n",
    "\n",
    "def City_info(soup):\n",
    "    \n",
    "    ret_dict = {}\n",
    "    ret_dict['city'] = soup.h1.get_text()\n",
    "    \n",
    "    \n",
    "    if soup.select_one('.mergedrow:-soup-contains(\"Mayor\")>.infobox-label') != None:\n",
    "        i = soup.select_one('.mergedrow:-soup-contains(\"Mayor\")>.infobox-label')\n",
    "        mayor_name_html = i.find_next_sibling()\n",
    "        mayor_name = unicodedata.normalize('NFKD',mayor_name_html.get_text())\n",
    "        ret_dict['mayor']  = mayor_name\n",
    "    \n",
    "    if soup.select_one('.mergedrow:-soup-contains(\"City\")>.infobox-label') != None:\n",
    "        j =  soup.select_one('.mergedrow:-soup-contains(\"City\")>.infobox-label')\n",
    "        area = j.find_next_sibling('td').get_text()\n",
    "        ret_dict['city_size'] = unicodedata.normalize('NFKD',area)\n",
    "\n",
    "    if soup.select_one('.mergedtoprow:-soup-contains(\"Elevation\")>.infobox-data') != None:\n",
    "        k = soup.select_one('.mergedtoprow:-soup-contains(\"Elevation\")>.infobox-data')\n",
    "        elevation_html = k.get_text()\n",
    "        ret_dict['elevation'] = unicodedata.normalize('NFKD',elevation_html)\n",
    "    \n",
    "    if soup.select_one('.mergedtoprow:-soup-contains(\"Population\")') != None:\n",
    "        l = soup.select_one('.mergedtoprow:-soup-contains(\"Population\")')\n",
    "        c_pop = l.findNext('td').get_text()\n",
    "        ret_dict['city_population'] = c_pop\n",
    "    \n",
    "    if soup.select_one('.infobox-label>[title^=Urban]') != None:\n",
    "        m = soup.select_one('.infobox-label>[title^=Urban]')\n",
    "        u_pop = m.findNext('td')\n",
    "        ret_dict['urban_population'] = u_pop.get_text()\n",
    "\n",
    "    if soup.select_one('.infobox-label>[title^=Metro]') != None:\n",
    "        n = soup.select_one('.infobox-label>[title^=Metro]')\n",
    "        m_pop = n.findNext('td')\n",
    "        ret_dict['metro_population'] = m_pop.get_text()\n",
    "    \n",
    "    if soup.select_one('.latitude') != None:\n",
    "        o = soup.select_one('.latitude')\n",
    "        ret_dict['lat'] = o.get_text()\n",
    "\n",
    "    if soup.select_one('.longitude') != None:    \n",
    "        p = soup.select_one('.longitude')\n",
    "        ret_dict['long'] = p.get_text()\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "list_of_city_info = []\n",
    "for city in cities:\n",
    "    url = 'https://en.wikipedia.org/wiki/{}'.format(city)\n",
    "    web = requests.get(url,'html.parser')\n",
    "    soup = bs(web.content)\n",
    "    list_of_city_info.append(City_info(soup))\n",
    "df_cities = pd.DataFrame(list_of_city_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1ec30-d550-47d7-9273-6c01bd445bd5",
   "metadata": {},
   "source": [
    "Reading the information about the airports from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07081969-8444-453c-8f70-8c0114ecf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data from csv file for information on the airports of the cities\n",
    "import pandas as pd\n",
    "\n",
    "airports_cities = (\n",
    "pd.read_csv('API-GANS/airports.csv')\n",
    "    .query('type == \"large_airport\"')\n",
    "    .filter(['name','latitude_deg','longitude_deg','iso_country','iso_region','municipality','gps_code','iata_code'])\n",
    "    .rename(columns={'gps_code':'icao_code'})\n",
    "    .assign(municipality_iso_country = lambda x: x['municipality'] + ',' + x['iso_country'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12310b1b-569c-4243-bbca-a9d1df001375",
   "metadata": {},
   "source": [
    "Get information about the time of Sunrise or Sunset of a given location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a14330ba-14c2-4107-99a4-1b2f4b65d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on Sun's position in city\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "tomorrows_date = (datetime.now() + timedelta(days=1))\n",
    "latitude =\"6,9\"\n",
    "API_Key=\"API-Key\"\n",
    "#Cologne\n",
    "url = \"https://geo-services-by-mvpc-com.p.rapidapi.com/sun_positions\"\n",
    "querystring = {\"location\":latitude,\"date\":tomorrows_date}\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"geo-services-by-mvpc-com.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\":API_Key\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "import pandas as pd\n",
    "sun_data_c= pd.json_normalize(response.json()['data'])\n",
    "# next step: select the columns you want to incude on your database\n",
    "sun_data_c = (\n",
    "sun_data_c\n",
    "    .filter(['sunrise','sunset','dawn','dusk','night','goldenHour' ])\n",
    "    .assign(city = 'Colonge,DE')\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4819dc-e239-4a1d-870a-3d6252b9ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "tomorrows_date = (datetime.now() + timedelta(days=1))\n",
    "API_Key=\"API_key\"\n",
    "latitude =\"9,1\"\n",
    "#Stuttgart\n",
    "url = \"https://geo-services-by-mvpc-com.p.rapidapi.com/sun_positions\"\n",
    "querystring = {\"location\":latitude,\"date\":tomorrows_date}\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"geo-services-by-mvpc-com.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\":API_Key\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "import pandas as pd\n",
    "sun_data_s= pd.json_normalize(response.json()['data'])\n",
    "# next step: select the columns you want to incude on your database\n",
    "sun_data_s= (\n",
    "    sun_data_s\n",
    "        .filter(['sunrise','sunset','dawn','dusk','night','goldenHour' ])\n",
    "        .assign(city = 'Stuttgart,DE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc761a4-28e1-47bf-a1a0-66eb8c5ad1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on Sun's position in city\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "tomorrows_date = (datetime.now() + timedelta(days=1))\n",
    "API_Key=\"API_key\"\n",
    "latitude =\"8,68\"\n",
    "#Frankfurt\n",
    "url = \"https://geo-services-by-mvpc-com.p.rapidapi.com/sun_positions\"\n",
    "querystring = {\"location\":latitude,\"date\":tomorrows_date}\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"geo-services-by-mvpc-com.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\":API_Key\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "import pandas as pd\n",
    "sun_data_f= pd.json_normalize(response.json()['data'])\n",
    "# next step: select the columns you want to incude on your database\n",
    "sun_data_f= (\n",
    "sun_data_f\n",
    "    .filter(['sunrise','sunset','dawn','dusk','night','goldenHour' ])\n",
    "    .assign(city = 'Frankfurt,DE')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66036275-d572-4675-8b9b-d678145bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on Sun's position in city\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "tomorrows_date = (datetime.now() + timedelta(days=1))\n",
    "API_Key=\"API_key\"\n",
    "latitude =\"13,4\"\n",
    "#Berlin\n",
    "url = \"https://geo-services-by-mvpc-com.p.rapidapi.com/sun_positions\"\n",
    "querystring = {\"location\":latitude,\"date\":tomorrows_date}\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"geo-services-by-mvpc-com.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\":API_Key\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "import pandas as pd\n",
    "sun_data_b= pd.json_normalize(response.json()['data'])\n",
    "# next step: select the columns you want to incude on your database\n",
    "sun_data_b = (\n",
    "sun_data_b\n",
    "    .filter(['sunrise','sunset','dawn','dusk','night','goldenHour' ])\n",
    "    .assign(city = 'Berlin,DE')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9e222-4f0b-48df-8510-0687a2dcd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "tomorrows_date = (datetime.now() + timedelta(days=1))\n",
    "API_Key=\"API_key\"\n",
    "latitude =\"11,5\"\n",
    "#Munich\n",
    "url = \"https://geo-services-by-mvpc-com.p.rapidapi.com/sun_positions\"\n",
    "querystring = {\"location\":latitude,\"date\":tomorrows_date}\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"geo-services-by-mvpc-com.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\": APIKey\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "import pandas as pd\n",
    "sun_data_m= pd.json_normalize(response.json()['data'])\n",
    "# next step: select the columns you want to incude on your database\n",
    "sun_data_m= (\n",
    "    sun_data_m\n",
    "        .filter(['sunrise','sunset','dawn','dusk','night','goldenHour' ])\n",
    "        .assign(city = 'Munich,DE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99de8c-30e1-485e-934d-43c7e8f59d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_Sun = [sun_data_s, sun_data_f, sun_data_m, sun_data_b, sun_data_k ]\n",
    "DE_Sundata = pd.concat(DE_Sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a74c3c-4749-4d2e-8635-b21ff400744a",
   "metadata": {},
   "source": [
    "Insert the information into the local database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bf1e8-4af7-4e9f-b80f-bb17bd4da498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the local databse\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "schema=\"name_of_schema\"\n",
    "host=\"host_name\"\n",
    "user=\"user_name\"\n",
    "password=\"user_password\"\n",
    "port=3306\n",
    "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4a38e-234b-496e-8820-d27f26f08d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create cities table and on execution the second time append the data in the existing database\n",
    "cities.dropna().to_sql('cities', con=con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf03946-762e-4787-a3c6-70249fb39f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create airports cities table and on execution the second time append the data in the existing database\n",
    "airports_cities.dropna().to_sql('airports_cities', if_exists='append', con=con, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db2711-76d3-4c05-8c6b-5e340c7f2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create weather table and on execution the second time append the data in the existing database\n",
    "weather_result.assign(datetime = lambda x: pd.to_datetime(x['datetime'])).to_sql('weather', if_exists='append', con=con, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c50056-5954-4c05-a380-536dad69d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrivals table and on execution the second time append the data in the existing database\n",
    "import numpy as np\n",
    "(\n",
    "arrivals_DE\n",
    "    .replace({np.nan},'unknown')\n",
    "    .assign(sched_arr_loc_time = lambda x: pd.to_datetime(x['sched_arr_loc_time']))\n",
    "    .to_sql('arrivals', if_exists='append', con=con, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f169359-19ce-47fb-a851-a31ca94b7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sundata table and on execution the second time append the data in the existing database\n",
    "DE_Sundata.dropna().to_sql('sundata', if_exists='append', con=con, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15386979-6330-4194-a74f-5eff99a322fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AWS database instance to insert data from the csv file\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "schema=\"schema_name\"\n",
    "host=\"Endpoint from AWS\"\n",
    "user=\"user_name\"\n",
    "password=\"Password\"\n",
    "port=3306\n",
    "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
    "\n",
    "(\n",
    " airports_cities.dropna().to_sql('airports', if_exists='append', con=con, index=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
