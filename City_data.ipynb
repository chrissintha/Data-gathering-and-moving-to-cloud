{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6208b04-ccad-44a3-9b83-d6bae129a508",
   "metadata": {},
   "source": [
    "Demographics information about the cities from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802ca27-6466-4edf-9696-2061e408f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Beautiful Soup to scrape data from the internet(Wikipedia pages for infomation on the cities)\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "cities = ['Berlin','Frankfurt','Stuttgart','Munich','Cologne']\n",
    "\n",
    "def City_info(soup):\n",
    "    \n",
    "    ret_dict = {}\n",
    "    ret_dict['city'] = soup.h1.get_text()\n",
    "    \n",
    "    \n",
    "    if soup.select_one('.mergedrow:-soup-contains(\"Mayor\")>.infobox-label') != None:\n",
    "        i = soup.select_one('.mergedrow:-soup-contains(\"Mayor\")>.infobox-label')\n",
    "        mayor_name_html = i.find_next_sibling()\n",
    "        mayor_name = unicodedata.normalize('NFKD',mayor_name_html.get_text())\n",
    "        ret_dict['mayor']  = mayor_name\n",
    "    \n",
    "    if soup.select_one('.mergedrow:-soup-contains(\"City\")>.infobox-label') != None:\n",
    "        j =  soup.select_one('.mergedrow:-soup-contains(\"City\")>.infobox-label')\n",
    "        area = j.find_next_sibling('td').get_text()\n",
    "        ret_dict['city_size'] = unicodedata.normalize('NFKD',area)\n",
    "\n",
    "    if soup.select_one('.mergedtoprow:-soup-contains(\"Elevation\")>.infobox-data') != None:\n",
    "        k = soup.select_one('.mergedtoprow:-soup-contains(\"Elevation\")>.infobox-data')\n",
    "        elevation_html = k.get_text()\n",
    "        ret_dict['elevation'] = unicodedata.normalize('NFKD',elevation_html)\n",
    "    \n",
    "    if soup.select_one('.mergedtoprow:-soup-contains(\"Population\")') != None:\n",
    "        l = soup.select_one('.mergedtoprow:-soup-contains(\"Population\")')\n",
    "        c_pop = l.findNext('td').get_text()\n",
    "        ret_dict['city_population'] = c_pop\n",
    "    \n",
    "    if soup.select_one('.infobox-label>[title^=Urban]') != None:\n",
    "        m = soup.select_one('.infobox-label>[title^=Urban]')\n",
    "        u_pop = m.findNext('td')\n",
    "        ret_dict['urban_population'] = u_pop.get_text()\n",
    "\n",
    "    if soup.select_one('.infobox-label>[title^=Metro]') != None:\n",
    "        n = soup.select_one('.infobox-label>[title^=Metro]')\n",
    "        m_pop = n.findNext('td')\n",
    "        ret_dict['metro_population'] = m_pop.get_text()\n",
    "    \n",
    "    if soup.select_one('.latitude') != None:\n",
    "        o = soup.select_one('.latitude')\n",
    "        ret_dict['lat'] = o.get_text()\n",
    "\n",
    "    if soup.select_one('.longitude') != None:    \n",
    "        p = soup.select_one('.longitude')\n",
    "        ret_dict['long'] = p.get_text()\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "list_of_city_info = []\n",
    "for city in cities:\n",
    "    url = 'https://en.wikipedia.org/wiki/{}'.format(city)\n",
    "    web = requests.get(url,'html.parser')\n",
    "    soup = bs(web.content)\n",
    "    list_of_city_info.append(City_info(soup))\n",
    "df_cities = pd.DataFrame(list_of_city_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
